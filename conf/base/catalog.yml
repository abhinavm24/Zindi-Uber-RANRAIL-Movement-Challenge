# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in the kedro docs under `Accessing data`
# You can access the kedro docs by running `kedro docs`

#
# An example data set definition can look as follows:
#
#cars.csv:
#  type: CSVLocalDataSet # https://kedro.readthedocs.io/en/latest/kedro.io.CSVLocalDataSet.html
#  filepath: data/01_raw/company/cars.csv
#  load_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
#    sep: ','
#    skiprows: 0
#    # skipfooter: 1
#    # engine: python  # Some of the features including skipfooter is only available in python engine
#    engine: c  # This is a faster option
#    na_values: ['#NA', 'NA']
#  save_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html
#    index: False
#    date_format: '%Y-%m-%d %H:%M'
#    decimal: '.'
#
#cars.csv.s3:
#  type: CSVS3DataSet # https://kedro.readthedocs.io/en/latest/kedro.io.CSVS3DataSet.html
#  filepath: data/02_intermediate/company/cars.csv
#  bucket_name: my_bucket
#  credentials: dev_s3
#  load_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html
#    sep: ','
#    skiprows: 5
#    skipfooter: 1
#    na_values: ['#NA', 'NA']
#    index: False
#  save_args: # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html
#    index: False
#    date_format: '%Y-%m-%d %H:%M'
#    decimal: '.'
#
#cars.hdf:
#  type: HDFLocalDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.HDFLocalDataSet.html
#  filepath: data/02_intermediate/cars.hdf
#  key: name
#  load_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_hdf.html
#    columns: ['engine', 'name']
#  save_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_hdf.html
#    mode: 'w'  # Overwrite even when the file already exists
#    # mode: 'a'  # Appends or creates a file
#    # mode: '+r'  # Appends an existing file
#    dropna: True
#
#cars.hdf.s3:
#  type: HDFS3DataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.HDFS3DataSet.html
#  filepath: data/02_intermediate/cars.hdf
#  bucket_name: my_bucket
#  key: hdf_key
#  credentials: dev_s3
#  load_args:
#  save_args:
#
#cars.parquet:
#  type: ParquetLocalDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.ParquetLocalDataSet.html
#  filepath: data/02_intermediate/cars.parquet
#  load_args:
#    columns: ['name', 'gear','disp', 'wt']
#  save_args:
#     compression: 'GZIP'
#
#cars.sql:
#  type: SQLTableDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.SQLTableDataSet.html
#  credentials: dev_postgres
#  table_name: cars
#  load_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html
#    index_col: ['name']
#    columns: ['name', 'gear']
#  save_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html
#    if_exists: 'replace'
#    # if_exists: 'fail'
#    # if_exists: 'append'
#
#cars.sql.query:
#  type: SQLQueryDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.SQLQueryDataSet.html
#  credentials: dev_postgres
#  sql: 'select * from cars where gear=4'
#  load_args:  # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_query.html
#    index_col: ['name']
#
#
## Templating and reuse
#
#_csv: &csv
#  type: kedro.contrib.io.pyspark.spark_data_set.SparkDataSet
#  file_format: 'csv'
#  load_args:
#    header: True
#    inferSchema: False
#
#raw_banana_trials:
#  <<: *csv
#  filepath: "s3a://supermarket/01_raw/Banana/trials.csv"
#
## Transcoding
#
#cars@parquet:
#  type: ParquetLocalDataSet  # https://kedro.readthedocs.io/en/latest/kedro.io.ParquetLocalDataSet.html
#  filepath: data/02_intermediate/cars.parquet
#  load_args:
#    columns: ['name', 'gear','disp', 'wt']
#  save_args:
#     compression: 'GZIP'
#
#cars@spark:
#  type: kedro.contrib.io.pyspark.SparkDataSet  # https://kedro.readthedocs.io/en/latest/kedro.contrib.io.pyspark.SparkDataSet.html
#  filepath: data/02_intermediate/cars.parquet



# This is a data set used by the example pipeline provided with the projected
# template. Please feel free to remove it once you remove the example pipeline.
example_iris_data:
  type: CSVLocalDataSet
  filepath: data/01_raw/iris.csv 
  
  
###################################################

# Models
dummy_model.pkl:
 type: PickleLocalDataSet
 filepath: data/06_models/dummy_model.pkl
 backend: joblib
 versioned: true
 
nb_model.pkl:
 type: PickleLocalDataSet
 filepath: data/06_models/nb_model.pkl
 backend: joblib
 versioned: true
 
 
# submissions
nb_submission_data:
  type: CSVLocalDataSet
  filepath: data/07_model_output/NBSubmission.csv
  versioned: true
  
svd_submission_data:
  type: CSVLocalDataSet
  filepath: data/07_model_output/SVDSubmission.csv
  versioned: true
  
nmf_submission_data:
  type: CSVLocalDataSet
  filepath: data/07_model_output/NMFSubmission.csv
  versioned: true
  
knn_submission_data:
  type: CSVLocalDataSet
  filepath: data/07_model_output/SVDSubmission.csv
  versioned: true
  
dummy_submission_data:
  type: CSVLocalDataSet
  filepath: data/07_model_output/DummySubmission.csv
  versioned: trues
  
# Root files
sample_submission_data:
  type: CSVLocalDataSet
  filepath: data/01_raw/SampleSubmission.csv
  
transformed_sample_submission_data:
  type: ParquetLocalDataSet
  filepath: data/02_intermediate/TransformedSampleSubmission.parquet
  
merged_sample_submission_data:
  type: ParquetLocalDataSet
  filepath: data/03_primary/MergedSampleSubmission.parquet
  
sparse_data:
  type: ParquetLocalDataSet
  filepath: data/05_model_input/SparseData.parquet
  
transformed_sparse_data:
  type: ParquetLocalDataSet
  filepath: data/05_model_input/TransformedSparseData.parquet
  
transformed_sparse_data_index:
  type: ParquetLocalDataSet
  filepath: data/05_model_input/TransformedSparseIndex.parquet
  
train_variabledefinitions_data:
  type: CSVLocalDataSet
  filepath: data/01_raw/train_VariableDefinitions.csv
  
train_data:
  type: CSVLocalDataSet
  filepath: data/01_raw/train.csv
  
vehicles2016_2018_data:
  type: CSVLocalDataSet
  filepath: data/01_raw/Vehicles2016_2018.csv
  
  
# SANRAL Data
sanral_zip_load:
  type: hackathon.io.ZipDataSet
  filepath: data/01_raw/SANRAL.zip
  versioned: false

  
sanral_zip_save:
  type: hackathon.io.ZipDataSet
  filepath: data/02_intermediate/SANRAL
  versioned: false
  
injuries2016_2018_data:
  type: CSVLocalDataSet
  filepath: data/02_intermediate/SANRAL/Injuries2016_2018.csv
  

vehicles2016_2018_data:
  type: CSVLocalDataSet
  filepath: data/02_intermediate/SANRAL/Vehicles2016_2018.csv

# ROAD_SEGMENTS
road_segments_zip_load:
  type: hackathon.io.ZipDataSet
  filepath: data/01_raw/road_segments.zip
  versioned: false

  
road_segments_zip_save:
  type: hackathon.io.ZipDataSet
  filepath: data/02_intermediate/road_segments
  versioned: false
  
  
# UBER MOVEMENT Data
uber_movement_data_zip_load:
  type: hackathon.io.ZipDataSet
  filepath: data/01_raw/Uber movement data.zip
  versioned: false

  
uber_movement_data_zip_save:
  type: hackathon.io.ZipDataSet
  filepath: data/02_intermediate/Uber movement data
  versioned: false
  
  
  
  


